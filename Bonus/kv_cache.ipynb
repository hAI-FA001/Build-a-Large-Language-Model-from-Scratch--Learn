{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4daeaf9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import tiktoken\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cb75af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deps.gpt_model import LayerNorm, GELU, FeedForward\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_len, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "\n",
    "        # old code\n",
    "        assert d_out % num_heads == 0, 'd_out must be divisible by num_heads'\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "\n",
    "        self.W_q = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_k = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_v = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            'mask',\n",
    "            torch.triu(torch.ones(context_len, context_len), diagonal=1),\n",
    "            persistent=False\n",
    "        )\n",
    "\n",
    "        # new code\n",
    "        self.register_buffer('cache_k', None, persistent=False)\n",
    "        self.register_buffer('cache_v', None, persistent=False)\n",
    "        self.ptr_cur_pos = 0\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # old code\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        queries = self.W_q(x)\n",
    "        keys = self.W_k(x)\n",
    "        values = self.W_v(x)\n",
    "\n",
    "        # (b, n, d) -> (b, n, heads, head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # new code\n",
    "        if self.cache_k is None:\n",
    "            self.cache_k, self.cache_v = keys, values\n",
    "        else:\n",
    "            self.cache_k = torch.cat([self.cache_k, keys], dim=1)\n",
    "            self.cache_v = torch.cat([self.cache_v, values], dim=1)\n",
    "        \n",
    "        keys, values = self.cache_k, self.cache_v\n",
    "\n",
    "        # old code\n",
    "        keys = keys.transpose(1,2)\n",
    "        queries = queries.transpose(1,2)\n",
    "        values = values.transpose(1,2)\n",
    "\n",
    "        attn_scores = queries @ keys.transpose(2,3)\n",
    "\n",
    "        # new code\n",
    "        num_tokens_q = queries.shape[-2]\n",
    "        num_tokens_k = keys.shape[-2]\n",
    "        mask_bool = self.mask.bool()\n",
    "        # original was: mask_bool[:num_tokens_q, :num_tokens_k]\n",
    "        mask_bool = mask_bool[\n",
    "            self.ptr_cur_pos:self.ptr_cur_pos+num_tokens_q,\n",
    "            :num_tokens_k\n",
    "        ]\n",
    "        self.ptr_cur_pos += num_tokens_q\n",
    "\n",
    "        # old code\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        context_vec = (attn_weights @ values).transpose(1,2)\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec)\n",
    "\n",
    "        return context_vec\n",
    "    \n",
    "    # new code\n",
    "    def reset_cache(self):\n",
    "        self.cache_k, self.cache_v = None, None\n",
    "        self.ptr_cur_pos = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a87b28d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use new MHA\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attn = MultiHeadAttention(\n",
    "            d_in=cfg['emb_dim'],\n",
    "            d_out=cfg['emb_dim'],\n",
    "            context_len=cfg['context_len'],\n",
    "            num_heads=cfg['n_heads'],\n",
    "            dropout=cfg['drop_rate'],\n",
    "            qkv_bias=cfg['qkv_bias'],\n",
    "        )\n",
    "\n",
    "        self.ff = FeedForward(cfg)\n",
    "        \n",
    "        self.norm1 = LayerNorm(cfg['emb_dim'])\n",
    "        self.norm2 = LayerNorm(cfg['emb_dim'])\n",
    "\n",
    "        self.drop_shortcut = nn.Dropout(cfg['drop_rate'])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.attn(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e673b61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        \n",
    "        # old code\n",
    "        self.tok_emb = nn.Embedding(cfg['vocab_size'], cfg['emb_dim'])\n",
    "        self.pos_emb = nn.Embedding(cfg['context_len'], cfg['emb_dim'])\n",
    "        self.drop_emb = nn.Dropout(cfg['drop_rate'])\n",
    "\n",
    "        # slightly new code\n",
    "        self.trf_blocks = nn.ModuleList([\n",
    "            TransformerBlock(cfg) for _ in range(cfg['n_layers'])\n",
    "        ])\n",
    "        \n",
    "        self.cur_pos = 0\n",
    "\n",
    "        # old code\n",
    "        self.final_norm = LayerNorm(cfg['emb_dim'])\n",
    "        self.out_head = nn.Linear(cfg['emb_dim'], cfg['vocab_size'], bias=False)\n",
    "    \n",
    "    def forward(self, in_idx):\n",
    "        # old code\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embs = self.tok_emb(in_idx)\n",
    "        \n",
    "        # new code\n",
    "        # before: torch.arange(seq_len, ...)\n",
    "        pos_ids = torch.arange(\n",
    "            self.cur_pos, self.cur_pos + seq_len,\n",
    "            device=in_idx.device,\n",
    "            dtype=torch.long,\n",
    "        )\n",
    "        self.cur_pos += seq_len\n",
    "        pos_embs = self.pos_emb(pos_ids).unsqueeze(0)\n",
    "\n",
    "        # old code\n",
    "        x = tok_embs + pos_embs\n",
    "        x = self.drop_emb(x)\n",
    "\n",
    "        # new code\n",
    "        for blk in self.trf_blocks:\n",
    "            x = blk(x)\n",
    "\n",
    "        # old code\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    # new code\n",
    "    def reset_kv_cache(self):\n",
    "        for blk in self.trf_blocks:\n",
    "            blk.attn.reset_cache()\n",
    "        self.cur_pos = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eabffa33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple_cached(\n",
    "        model, idx, max_new_tokens,\n",
    "        context_size=None,\n",
    "):\n",
    "    model.eval()\n",
    "    ctx_len = context_size or model.pos_emb.num_embeddings\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # initialize cache with full prompt\n",
    "        model.reset_kv_cache()\n",
    "        logits = model(idx[:, -ctx_len:])\n",
    "        \n",
    "        for _ in range(max_new_tokens):\n",
    "            next_idx = logits[:, -1].argmax(dim=-1, keepdim=True)\n",
    "            idx = torch.cat([idx, next_idx], dim=1)\n",
    "            \n",
    "            # feed only the new token\n",
    "            logits = model(next_idx)\n",
    "    \n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d7d73dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "                      IN\n",
      "==================================================\n",
      "\n",
      "Input text: Hello, I am\n",
      "Encoded: [15496, 11, 314, 716]\n",
      "Encoded shape: torch.Size([1, 4])\n",
      "\n",
      "==================================================\n",
      "                      OUT\n",
      "==================================================\n",
      "\n",
      "Output: tensor([[15496,    11,   314,   716, 27018, 24086, 47843, 30961, 42348,  7267,\n",
      "         49706, 43231, 47062, 34657, 18631, 49188, 43312, 45933, 23154, 15983,\n",
      "         10345, 16369, 46214, 22954, 34674, 21100,  4743, 14056, 42526,  6459,\n",
      "         12799,  5734, 49274,   136, 49294, 42900, 21193, 20463,  1018,  7864,\n",
      "         13895, 27167, 12810, 25727, 14388,   985, 15797, 24440, 18557, 48625,\n",
      "         10579,  4007, 11895, 45365, 19051,  1355, 47705,  5120, 32858, 49293,\n",
      "          5141, 22900, 36570, 22215, 16369, 25803,  9254, 33694, 23188, 21624,\n",
      "         12696,  1697, 12315, 23338,  1361, 49487, 27970, 21641, 28170, 36226,\n",
      "          8980, 34715, 15683, 21370,   829, 41165, 19250, 40921, 47972, 29169,\n",
      "         17681, 13937,   719,  7781, 46519, 39685, 35637, 38254, 37355, 48054,\n",
      "          6960, 32389, 49945, 48307, 43363,  9451, 44360, 43623, 36233, 27350,\n",
      "         24597, 27334, 14212, 11243, 37010, 10723, 24492, 45795,  8216, 11362,\n",
      "         26754, 18517, 44987, 35224,  9535, 22574, 20579, 13134, 50072, 41511,\n",
      "         21603, 44476, 37708, 47619, 37179, 36111, 40412,  2529, 15569, 25531,\n",
      "         32132,  6450,  4333, 36408, 15648, 27025,  8393,   460,  5645,  8731,\n",
      "         29144, 30137, 32738, 37851,  5652, 22602, 36063, 15195, 19337, 25290,\n",
      "         22359, 14495, 46091, 31485,  9003, 42166, 33260,  5984, 20594, 28823,\n",
      "         29651, 43283,  1637,  3511, 43921, 28062,  3293, 20965, 38951, 34929,\n",
      "          6426, 41189, 36197, 50030, 37445, 50114,  6049, 21174, 30441, 49812,\n",
      "         35484, 28117,  4851, 17249, 17027, 17533, 14407, 25401, 41319,  9367,\n",
      "         28812,  6729, 43881, 38136]])\n",
      "Output length: 204\n",
      "Output text: Hello, I am Featureiman Byeswickattribute argue logger Normandy Compton analogous bore ITVEGIN ministriesysics Kle functional recountrictionchangingVirgin embarrassedgl Truthfoundation challenges essence specifically Absent� 421 lov Castro Fewug wins Hus Golemllyyll Fisher sim trimュPrintOracle Five purpose FO treacherous grades Be1001 proceed rodsurated kid unpredictableFans goddess recountPet hint mustard178 Pul Hotelennpdfنreenammu312 paintings waived mL¯¯¯¯ruly thumbsvles complains Nashville Principleulla misuseフ Fel act scored782 consensual Cemetery Highly Quietceivable Young FirearmsAimEconomic awokebered triangles Sturgeon distressed ArriBal bash influential Pa PinballWatchPublished rehearsal tone GPUAshOctober overriding AVGtra heck username acted Cellular unob smoked Ludwig nailedeersMarco Folk aluminium inj aggressionvic yogurt sand Poweriannopoulos Rise Frances Exec can endsWind glove disparity fuse contestants AngelesutilColumb Stra shallow WS tumor peersassian clauses airportBenef Pug AD similarities commentator morale obnoxious money yourselfPolitics feder substotorCollegekeye revenue moistur allergy glared Kosovo mainline severeDan scrutin aph bananaseenthFClatedzi bored Phantom OTHERazel scanemployment snow fuelingeworld\n",
      "Took 15.59s\n",
      "13 tokens/sec\n"
     ]
    }
   ],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,     # Vocabulary size\n",
    "    \"context_len\": 1024,  # Context length\n",
    "    \"emb_dim\": 768,          # Embedding dimension\n",
    "    \"n_heads\": 12,           # Number of attention heads\n",
    "    \"n_layers\": 12,          # Number of layers\n",
    "    \"drop_rate\": 0.1,        # Dropout rate\n",
    "    \"qkv_bias\": False        # Query-Key-Value bias\n",
    "}\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "start_context = 'Hello, I am'\n",
    "tokenizer = tiktoken.get_encoding('gpt2')\n",
    "encoded = tokenizer.encode(start_context)\n",
    "encoded_tensor = torch.tensor(encoded, device=device).unsqueeze(0)\n",
    "\n",
    "print(f'\\n{'='*50}\\n{' '*22}IN\\n{'='*50}\\n')\n",
    "print(f'Input text: {start_context}')\n",
    "print(f'Encoded: {encoded}')\n",
    "print(f'Encoded shape: {encoded_tensor.shape}')\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "token_ids = generate_text_simple_cached(\n",
    "    model,\n",
    "    encoded_tensor,\n",
    "    max_new_tokens=200,\n",
    ")\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "decoded = tokenizer.decode(token_ids.squeeze(0).tolist())\n",
    "print(f'\\n{'='*50}\\n{' '*22}OUT\\n{'='*50}\\n')\n",
    "print(f'Output: {token_ids}')\n",
    "print(f'Output length: {len(token_ids[0])}')\n",
    "print(f'Output text: {decoded}')\n",
    "\n",
    "print(f'Took {end-start:.2f}s')\n",
    "toks_per_sec = len(token_ids[0]) / (end-start)\n",
    "print(f'{int(toks_per_sec)} tokens/sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423e5262",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "code-book",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
