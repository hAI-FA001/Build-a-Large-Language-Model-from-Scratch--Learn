{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4272b6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1061849",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deps.other_components import SiLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ee19f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward_MoE(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.num_experts_per_tok = cfg['n_experts_per_tok']\n",
    "        self.num_experts = cfg['n_experts']\n",
    "\n",
    "        self.gate = nn.Linear(cfg['emb_dim'], cfg['n_experts'], bias=False, dtype=cfg['dtype'])\n",
    "\n",
    "        meta_device = torch.device('meta')  # to reduce memory when loading weights\n",
    "        self.fc1 = nn.ModuleList([\n",
    "            nn.Linear(cfg['emb_dim'], cfg['moe_intermediate_size'], bias=False, dtype=cfg['dtype'],\n",
    "                    device=meta_device)\n",
    "            for _ in range(cfg['n_experts'])\n",
    "        ])\n",
    "        self.fc2 = nn.ModuleList([\n",
    "            nn.Linear(cfg['emb_dim'], cfg['moe_intermediate_size'], bias=False, dtype=cfg['dtype'],\n",
    "                    device=meta_device)\n",
    "            for _ in range(cfg['n_experts'])\n",
    "        ])\n",
    "        self.fc3 = nn.ModuleList([\n",
    "            nn.Linear(cfg['moe_intermediate_size'], cfg['emb_dim'], bias=False, dtype=cfg['dtype'],\n",
    "                    device=meta_device)\n",
    "            for _ in range(cfg['n_experts'])\n",
    "        ])\n",
    "        self.silu = SiLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # (batch, seq_len, emb_dim) -> (batch, seq_len, n_experts)\n",
    "        scores = self.gate(x)\n",
    "        topk_scores, topk_idxs = torch.topk(scores, k=self.num_experts_per_tok, dim=-1)\n",
    "        topk_probas = torch.softmax(topk_scores, dim=-1)\n",
    "        \n",
    "        # (batch, seq_len, emb_dim)\n",
    "        y = torch.zeros_like(x)\n",
    "\n",
    "        for i in range(self.num_experts_per_tok):\n",
    "            # work on ith entry in top-k\n",
    "\n",
    "            # (batch, seq_len, n_experts) -> (batch, seq_len)\n",
    "            expert_idxs = topk_idxs[..., i]\n",
    "            # (batch, seq_len, n_experts) -> (batch, seq_len, 1)\n",
    "            expert_proba = topk_probas[..., i].unsqueeze(-1)\n",
    "\n",
    "            # each expert processes only assigned tokens\n",
    "            for e in range(self.num_experts):\n",
    "                # (batch, seq_len)\n",
    "                mask = (expert_idxs == e)\n",
    "                # check if any token in any batch is assigned to this expert\n",
    "                if mask.any():\n",
    "                    # (batch, seq_len, emb_dim) -> (n_tokens_e, emb_dim)\n",
    "                    # first 2 dims are indexed by mask\n",
    "                    selected = x[mask]\n",
    "                    \n",
    "                    # SwiGLU\n",
    "                    # (n_tokens_e, emb_dim) -> (n_tokens_e, moe_hidden)\n",
    "                    hidden = self.silu(self.fc1[e](selected)) * self.fc2[e](selected)\n",
    "                    # (n_tokens_e, moe_hidden) -> (n_tokens_e, emb_dim)\n",
    "                    out = self.fc3[e](hidden)\n",
    "                    y[mask] += expert_proba[mask] * out\n",
    "            \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29fe783b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deps.other_components import RMSNorm_Qwen\n",
    "from deps.other_components import precompute_rope_params, compute_rope\n",
    "from deps.other_components import GroupedQueryAttention_Qwen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b753235c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attn = GroupedQueryAttention_Qwen(\n",
    "            d_in=cfg['emb_dim'],\n",
    "            num_heads=cfg['n_heads'],\n",
    "            head_dim=cfg['head_dim'],\n",
    "            num_kv_groups=cfg['n_kv_groups'],\n",
    "            dtype=cfg['dtype']\n",
    "        )\n",
    "\n",
    "        self.ff = FeedForward_MoE(cfg)\n",
    "        self.norm1 = RMSNorm_Qwen(cfg['emb_dim'], eps=1e-6)\n",
    "        self.norm2 = RMSNorm_Qwen(cfg['emb_dim'], eps=1e-6)\n",
    "\n",
    "    def forward(self, x, mask, cos, sin):\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.attn(x, mask, cos, sin)\n",
    "        x = x + shortcut\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = x + shortcut\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "838d359e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qwen3MoEModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "\n",
    "        self.tok_emb = nn.Embedding(cfg['vocab_size'], cfg['emb_dim'], dtype=cfg['dtype'])\n",
    "        self.trf_blocks = nn.ModuleList([\n",
    "            TransformerBlock(cfg)\n",
    "            for _ in range(cfg['n_layers'])\n",
    "        ])\n",
    "\n",
    "        self.final_norm = RMSNorm_Qwen(cfg['emb_dim'])\n",
    "        self.out_head = nn.Linear(cfg['emb_dim'], cfg['emb_dim'], bias=False, dtype=cfg['dtype'])\n",
    "\n",
    "        cos, sin = precompute_rope_params(\n",
    "            cfg['head_dim'] if cfg['head_dim'] is not None else (cfg['emb_dim'] // cfg['n_heads']),\n",
    "            cfg['rope_base'],\n",
    "            cfg['context_len'],\n",
    "        )\n",
    "\n",
    "        self.register_buffer('cos', cos, persistent=True)\n",
    "        self.register_buffer('sin', sin, persistent=True)\n",
    "        self.cfg = cfg\n",
    "    \n",
    "    def forward(self, in_idx):\n",
    "        tok_embs = self.tok_emb(in_idx)\n",
    "        x = tok_embs\n",
    "\n",
    "        num_tokens = x.shape[1]\n",
    "        mask = torch.triu(\n",
    "            torch.ones(num_tokens, num_tokens, device=x.device, dtype=torch.bool),\n",
    "            diagonal=1,\n",
    "        )\n",
    "\n",
    "        for block in self.trf_blocks:\n",
    "            x = block(x, mask, self.cos, self.sin)\n",
    "        \n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x.to(self.cfg['dtype']))\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "865a49ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "QWEN3_CONFIG = {\n",
    "    'vocab_size': 151_936,\n",
    "    'context_len': 262_144,\n",
    "    'emb_dim': 2048,\n",
    "    'n_heads': 32,\n",
    "    'n_layers': 48,\n",
    "    'head_dim': 128,\n",
    "    'n_kv_groups': 4,\n",
    "    'rope_base': 10_000_000.0,\n",
    "    'dtype': torch.bfloat16,\n",
    "    'n_experts': 128,\n",
    "    'n_experts_per_tok': 8,\n",
    "    'moe_intermediate_size': 768\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aef0fdd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify if it works\n",
    "\n",
    "TEST_QWEN3_CONFIG = {\n",
    "    'vocab_size': 64,\n",
    "    'context_len': 256,\n",
    "    'emb_dim': 1024,\n",
    "    'n_heads': 8,\n",
    "    'n_layers': 2,\n",
    "    'head_dim': 128,\n",
    "    'n_kv_groups': 2,\n",
    "    'rope_base': 10_000_000.0,\n",
    "    'dtype': torch.bfloat16,\n",
    "    'n_experts': 32,\n",
    "    'n_experts_per_tok': 8,\n",
    "    'moe_intermediate_size': 324\n",
    "}\n",
    "\n",
    "model = Qwen3MoEModel(TEST_QWEN3_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c96204b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.2949, -0.2734, -0.4121,  ..., -0.3066, -1.1406, -0.4082]]],\n",
       "       dtype=torch.bfloat16, grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.tensor([1,2,3]).unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "172a2a05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params: 70,129,152\n",
      "Total unique params: 70,063,616\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total params: {total_params:,}\")\n",
    "\n",
    "total_params = total_params - model.tok_emb.weight.numel()\n",
    "print(f\"Total unique params: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee20f83a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "code-book",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
