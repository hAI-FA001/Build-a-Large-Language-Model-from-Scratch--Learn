{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4272b6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1061849",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deps.other_components import SiLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ee19f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward_MoE(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.num_experts_per_tok = cfg['n_experts_per_tok']\n",
    "        self.num_experts = cfg['n_experts']\n",
    "\n",
    "        self.gate = nn.Linear(cfg['emb_dim'], cfg['n_experts'], bias=False, dtype=cfg['dtype'])\n",
    "\n",
    "        meta_device = torch.device('meta')  # to reduce memory when loading weights\n",
    "        self.fc1 = nn.ModuleList([\n",
    "            nn.Linear(cfg['emb_dim'], cfg['moe_intermediate_size'], bias=False, dtype=cfg['dtype'],\n",
    "                    device=meta_device)\n",
    "            for _ in range(cfg['n_experts'])\n",
    "        ])\n",
    "        self.fc2 = nn.ModuleList([\n",
    "            nn.Linear(cfg['emb_dim'], cfg['moe_intermediate_size'], bias=False, dtype=cfg['dtype'],\n",
    "                    device=meta_device)\n",
    "            for _ in range(cfg['n_experts'])\n",
    "        ])\n",
    "        self.fc3 = nn.ModuleList([\n",
    "            nn.Linear(cfg['moe_intermediate_size'], cfg['emb_dim'], bias=False, dtype=cfg['dtype'],\n",
    "                    device=meta_device)\n",
    "            for _ in range(cfg['n_experts'])\n",
    "        ])\n",
    "        self.silu = SiLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # (batch, seq_len, emb_dim) -> (batch, seq_len, n_experts)\n",
    "        scores = self.gate(x)\n",
    "        topk_scores, topk_idxs = torch.topk(scores, k=self.num_experts_per_tok, dim=-1)\n",
    "        topk_probas = torch.softmax(topk_scores, dim=-1)\n",
    "        \n",
    "        # (batch, seq_len, emb_dim)\n",
    "        y = torch.zeros_like(x)\n",
    "\n",
    "        for i in range(self.num_experts_per_tok):\n",
    "            # work on ith entry in top-k\n",
    "\n",
    "            # (batch, seq_len, n_experts) -> (batch, seq_len)\n",
    "            expert_idxs = topk_idxs[..., i]\n",
    "            # (batch, seq_len, n_experts) -> (batch, seq_len, 1)\n",
    "            expert_proba = topk_probas[..., i].unsqueeze(-1)\n",
    "\n",
    "            # each expert processes only assigned tokens\n",
    "            for e in range(self.num_experts):\n",
    "                # (batch, seq_len)\n",
    "                mask = (expert_idxs == e)\n",
    "                # check if any token in any batch is assigned to this expert\n",
    "                if mask.any():\n",
    "                    # (batch, seq_len, emb_dim) -> (n_tokens_e, emb_dim)\n",
    "                    # first 2 dims are indexed by mask\n",
    "                    selected = x[mask]\n",
    "                    \n",
    "                    # SwiGLU\n",
    "                    # (n_tokens_e, emb_dim) -> (n_tokens_e, moe_hidden)\n",
    "                    hidden = self.silu(self.fc1[e](selected)) * self.fc2[e](selected)\n",
    "                    # (n_tokens_e, moe_hidden) -> (n_tokens_e, emb_dim)\n",
    "                    out = self.fc3[e](hidden)\n",
    "                    y[mask] += expert_proba[mask] * out\n",
    "            \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29fe783b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deps.other_components import RMSNorm_Qwen\n",
    "from deps.other_components import precompute_rope_params, compute_rope\n",
    "from deps.other_components import GroupedQueryAttention_Qwen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b753235c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "code-book",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
