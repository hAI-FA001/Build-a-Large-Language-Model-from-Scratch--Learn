{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0111e91c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bytearray(b'This is some text.')\n",
      "[84, 104, 105, 115, 32, 105, 115, 32, 115, 111, 109, 101, 32, 116, 101, 120, 116, 46]\n"
     ]
    }
   ],
   "source": [
    "text = \"This is some text.\"\n",
    "byte_array = bytearray(text, 'utf-8')\n",
    "print(byte_array)\n",
    "print(list(byte_array))  # each integer corresponds to the byte value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ebc76ca",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "byte must be in range(0, 256)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# byte = 8 bits = 256 possible values, 0 to 255\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mbytearray\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m257\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)  \u001b[38;5;66;03m# gives error\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: byte must be in range(0, 256)"
     ]
    }
   ],
   "source": [
    "# byte = 8 bits = 256 possible values, 0 to 255\n",
    "print(bytearray(range(0, 257)))  # gives error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8c3b63a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000: !\n",
      "001: \"\n",
      "002: #\n",
      "003: $\n",
      "004: %\n",
      "005: &\n",
      "006: '\n",
      "007: (\n",
      "008: )\n",
      "009: *\n",
      "010: +\n",
      "011: ,\n",
      "012: -\n",
      "013: .\n",
      "014: /\n",
      "015: 0\n",
      "016: 1\n",
      "017: 2\n",
      "018: 3\n",
      "019: 4\n",
      "020: 5\n",
      "021: 6\n",
      "022: 7\n",
      "023: 8\n",
      "024: 9\n",
      "025: :\n",
      "026: ;\n",
      "027: <\n",
      "028: =\n",
      "029: >\n",
      "030: ?\n",
      "031: @\n",
      "032: A\n",
      "033: B\n",
      "034: C\n",
      "035: D\n",
      "036: E\n",
      "037: F\n",
      "038: G\n",
      "039: H\n",
      "040: I\n",
      "041: J\n",
      "042: K\n",
      "043: L\n",
      "044: M\n",
      "045: N\n",
      "046: O\n",
      "047: P\n",
      "048: Q\n",
      "049: R\n",
      "050: S\n",
      "051: T\n",
      "052: U\n",
      "053: V\n",
      "054: W\n",
      "055: X\n",
      "056: Y\n",
      "057: Z\n",
      "058: [\n",
      "059: \\\n",
      "060: ]\n",
      "061: ^\n",
      "062: _\n",
      "063: `\n",
      "064: a\n",
      "065: b\n",
      "066: c\n",
      "067: d\n",
      "068: e\n",
      "069: f\n",
      "070: g\n",
      "071: h\n",
      "072: i\n",
      "073: j\n",
      "074: k\n",
      "075: l\n",
      "076: m\n",
      "077: n\n",
      "078: o\n",
      "079: p\n",
      "080: q\n",
      "081: r\n",
      "082: s\n",
      "083: t\n",
      "084: u\n",
      "085: v\n",
      "086: w\n",
      "087: x\n",
      "088: y\n",
      "089: z\n",
      "090: {\n",
      "091: |\n",
      "092: }\n",
      "093: ~\n",
      "094: �\n",
      "095: �\n",
      "096: �\n",
      "097: �\n",
      "098: �\n",
      "099: �\n",
      "100: �\n",
      "101: �\n",
      "102: �\n",
      "103: �\n",
      "104: �\n",
      "105: �\n",
      "106: �\n",
      "107: �\n",
      "108: �\n",
      "109: �\n",
      "110: �\n",
      "111: �\n",
      "112: �\n",
      "113: �\n",
      "114: �\n",
      "115: �\n",
      "116: �\n",
      "117: �\n",
      "118: �\n",
      "119: �\n",
      "120: �\n",
      "121: �\n",
      "122: �\n",
      "123: �\n",
      "124: �\n",
      "125: �\n",
      "126: �\n",
      "127: �\n",
      "128: �\n",
      "129: �\n",
      "130: �\n",
      "131: �\n",
      "132: �\n",
      "133: �\n",
      "134: �\n",
      "135: �\n",
      "136: �\n",
      "137: �\n",
      "138: �\n",
      "139: �\n",
      "140: �\n",
      "141: �\n",
      "142: �\n",
      "143: �\n",
      "144: �\n",
      "145: �\n",
      "146: �\n",
      "147: �\n",
      "148: �\n",
      "149: �\n",
      "150: �\n",
      "151: �\n",
      "152: �\n",
      "153: �\n",
      "154: �\n",
      "155: �\n",
      "156: �\n",
      "157: �\n",
      "158: �\n",
      "159: �\n",
      "160: �\n",
      "161: �\n",
      "162: �\n",
      "163: �\n",
      "164: �\n",
      "165: �\n",
      "166: �\n",
      "167: �\n",
      "168: �\n",
      "169: �\n",
      "170: �\n",
      "171: �\n",
      "172: �\n",
      "173: �\n",
      "174: �\n",
      "175: �\n",
      "176: �\n",
      "177: �\n",
      "178: �\n",
      "179: �\n",
      "180: �\n",
      "181: �\n",
      "182: �\n",
      "183: �\n",
      "184: �\n",
      "185: �\n",
      "186: �\n",
      "187: �\n",
      "188: \u0000\n",
      "189: \u0001\n",
      "190: \u0002\n",
      "191: \u0003\n",
      "192: \u0004\n",
      "193: \u0005\n",
      "194: \u0006\n",
      "195: \u0007\n",
      "196:\n",
      "197: \t\n",
      "198: \n",
      "\n",
      "199: \u000b\n",
      "200: \f\n",
      "201: \n",
      "202: \u000e\n",
      "203: \u000f\n",
      "204: \u0010\n",
      "205: \u0011\n",
      "206: \u0012\n",
      "207: \u0013\n",
      "208: \u0014\n",
      "209: \u0015\n",
      "210: \u0016\n",
      "211: \u0017\n",
      "212: \u0018\n",
      "213: \u0019\n",
      "214: \u001a\n",
      "215: \u001b\n",
      "216: \u001c\n",
      "217: \u001d\n",
      "218: \u001e\n",
      "219: \u001f\n",
      "220:  \n",
      "221: \n",
      "222: �\n",
      "223: �\n",
      "224: �\n",
      "225: �\n",
      "226: �\n",
      "227: �\n",
      "228: �\n",
      "229: �\n",
      "230: �\n",
      "231: �\n",
      "232: �\n",
      "233: �\n",
      "234: �\n",
      "235: �\n",
      "236: �\n",
      "237: �\n",
      "238: �\n",
      "239: �\n",
      "240: �\n",
      "241: �\n",
      "242: �\n",
      "243: �\n",
      "244: �\n",
      "245: �\n",
      "246: �\n",
      "247: �\n",
      "248: �\n",
      "249: �\n",
      "250: �\n",
      "251: �\n",
      "252: �\n",
      "253: �\n",
      "254: �\n",
      "255: �\n",
      "256:  t\n",
      "257:  a\n",
      "258: he\n",
      "259: in\n",
      "260: re\n",
      "261: on\n",
      "262:  the\n",
      "263: er\n",
      "264:  s\n",
      "265: at\n",
      "266:  w\n",
      "267:  o\n",
      "268: en\n",
      "269:  c\n",
      "270: it\n",
      "271: is\n",
      "272: an\n",
      "273: or\n",
      "274: es\n",
      "275:  b\n",
      "276: ed\n",
      "277:  f\n",
      "278: ing\n",
      "279:  p\n",
      "280: ou\n",
      "281:  an\n",
      "282: al\n",
      "283: ar\n",
      "284:  to\n",
      "285:  m\n",
      "286:  of\n",
      "287:  in\n",
      "288:  d\n",
      "289:  h\n",
      "290:  and\n",
      "291: ic\n",
      "292: as\n",
      "293: le\n",
      "294:  th\n",
      "295: ion\n",
      "296: om\n",
      "297: ll\n",
      "298: ent\n",
      "299:  n\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "gpt2_tokenizer = tiktoken.get_encoding('gpt2')\n",
    "# bpe uses 0-255 to represent its first 256 single-character tokens\n",
    "for i in range(300):\n",
    "    print(f'{i:03}: {gpt2_tokenizer.decode([i])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7c2b89",
   "metadata": {},
   "source": [
    "### Idea\n",
    "- Identify frequent pairs of bytes\n",
    "- Replace pair with a new placeholder ID, e.g. 256 if we start with 0-255\n",
    "- Record in a lookup table, the size of the table is the vocabulary size\n",
    "- Repeat\n",
    "- Stop when no pair occurs more than once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b20361c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, deque\n",
    "from functools import lru_cache\n",
    "import json\n",
    "\n",
    "class BPETokenizerSimple:\n",
    "    def __init__(self):\n",
    "        self.vocab = {}  # ID -> token\n",
    "        self.inv_vocab = {}  # token -> ID\n",
    "        self.bpe_merges = {}  # e.g: {(id1, id2): merged_id}\n",
    "        # for OpenAI GPT-2 merges, e.g: {(str1, str2): rank}\n",
    "        # lower rank = higher priority\n",
    "        self.bpe_ranks = {}\n",
    "    \n",
    "    def train(self, text, vocab_size, allowed_special={'<|endoftext|>'}):\n",
    "        # replace space with Ġ (particularity of GPT-2 BPE)\n",
    "        # e.g: \"Hello world\" -> [\"Hello\", \"Ġworld\"]\n",
    "        # GPT-4 BPE would give [\"Hello\", \" world\"]\n",
    "        preproc_text = []\n",
    "        for i, ch in enumerate(text):\n",
    "            if ch == ' ' and i != 0:\n",
    "                preproc_text.append('Ġ')\n",
    "            if ch != ' ':\n",
    "                preproc_text.append(ch)\n",
    "        preproc_text = ''.join(preproc_text)\n",
    "\n",
    "        # start with first 0-255 ASCII\n",
    "        unique_chrs = {chr(i) for i in range(256)}\n",
    "        # add other chars, like Ġ\n",
    "        unique_chrs.extend(ch for ch in sorted(set(preproc_text)) if ch not in unique_chrs)\n",
    "        if 'Ġ' not in unique_chrs:\n",
    "            unique_chrs.append('Ġ')\n",
    "        \n",
    "        self.vocab = {i: ch for i, ch in enumerate(unique_chrs)}\n",
    "        self.inv_vocab = {ch: i for i, ch in self.vocab.items()}\n",
    "\n",
    "        if allowed_special:\n",
    "            for token in allowed_special:\n",
    "                # use N as new ID\n",
    "                new_id = len(self.vocab)\n",
    "                self.vocab[new_id] = token\n",
    "                self.inv_vocab[token] = new_id\n",
    "        \n",
    "        # tokenize\n",
    "        token_ids = [self.inv_vocab[ch] for ch in preproc_text]\n",
    "\n",
    "        # BPE\n",
    "        for new_id in range(len(self.vocab), vocab_size):\n",
    "            pair_id = self.find_freq_pair(token_ids, mode='most')\n",
    "            \n",
    "            # stop if no pair occurs more than once\n",
    "            if pair_id is None:\n",
    "                break\n",
    "            \n",
    "            token_ids = self.replace_pair(token_ids, pair_id, new_id)\n",
    "            self.bpe_merges[pair_id] = new_id\n",
    "        \n",
    "        # add to vocabulary\n",
    "        for (p0, p1), new_id in self.bpe_merges.items():\n",
    "            merged = self.vocab[p0] + self.vocab[p1]\n",
    "            self.vocab[new_id] = merged\n",
    "            self.inv_vocab[merged] = new_id\n",
    "\n",
    "    @staticmethod\n",
    "    def find_freq_pair(token_ids, mode='most'):\n",
    "        # pairs of (n, n+1)\n",
    "        pairs = Counter(zip(token_ids, token_ids[1:]))\n",
    "        \n",
    "        if not pairs:\n",
    "            return None\n",
    "        \n",
    "        # key is based on counts, stored in x[1]\n",
    "        if mode == 'most':\n",
    "            return max(pairs.items(), key=lambda x: x[1])[0]\n",
    "        elif mode == 'least':\n",
    "            return min(pairs.items(), key=lambda x: x[1])[0]\n",
    "        else:\n",
    "            raise ValueError('Invalid mode. Must be \"most\" or \"least\"')\n",
    "    \n",
    "    @staticmethod\n",
    "    def replace_pair(token_ids, pair_id, new_id):\n",
    "        dq = deque(token_ids)\n",
    "        replaced = []\n",
    "\n",
    "        while dq:\n",
    "            cur = dq.popleft()\n",
    "            # check if cur and cur+1 make the pair\n",
    "            if dq and (cur, dq[0]) == pair_id:\n",
    "                # replace cur and cur+1 with a single id\n",
    "                replaced.append(new_id)\n",
    "                dq.popleft()\n",
    "            else:\n",
    "                replaced.append(cur)\n",
    "        \n",
    "        return replaced\n",
    "\n",
    "    def encode(self, text, allowed_special=None):\n",
    "        import re\n",
    "\n",
    "        token_ids = []\n",
    "\n",
    "        if allowed_special is not None and len(allowed_special) > 0:\n",
    "            # escape special characters then merge each special with |\n",
    "            # creates a pattern like (special1|special2|special3|...)\n",
    "            special_regex_pattern = \"(\" + \\\n",
    "            \"|\".join(re.escape(tok) for tok in sorted(allowed_special, key=len, reverse=True)) \\\n",
    "            + \")\"\n",
    "\n",
    "            last_idx = 0\n",
    "            for match in re.finditer(special_regex_pattern, text):\n",
    "                # all characters before a special token\n",
    "                prefix = text[last_idx:match.start()]\n",
    "                # prefix requires no special handling\n",
    "                token_ids.extend(self.encode(prefix, allowed_special=None))\n",
    "\n",
    "                special_token = match.group(0)\n",
    "                if special_token in self.inv_vocab:\n",
    "                    token_ids.append(self.inv_vocab[special_token])\n",
    "                else:\n",
    "                    raise ValueError(f'Special token {special_token} not in vocabulary.')\n",
    "\n",
    "                last_idx = match.end()\n",
    "            \n",
    "            # all characters after last special token\n",
    "            text = text[last_idx:]\n",
    "\n",
    "            disallowed = [\n",
    "                tok for tok in self.inv_vocab\n",
    "                if tok.startswith('<|') and tok.endswith('|>') \\\n",
    "                and tok in text and tok not in allowed_special\n",
    "            ]\n",
    "\n",
    "            if disallowed:\n",
    "                raise ValueError(f'Disallowed special tokens found in text: {disallowed}')\n",
    "        \n",
    "        # text will be treated as having no special tokens at this point\n",
    "        tokens = []\n",
    "        lines =  text.split('\\n')\n",
    "        for i, line in enumerate(lines):\n",
    "            if i > 0:  # separate each line with newline\n",
    "                tokens.append('\\n')\n",
    "            \n",
    "            words = line.split()\n",
    "            for j, word in enumerate(words):\n",
    "                if j == 0 and i > 0:  # first word and not first line\n",
    "                    # separate words by Ġ\n",
    "                    tokens.append('Ġ'+word)\n",
    "                elif j == 0:  # first word and first line\n",
    "                    tokens.append(word)\n",
    "                else:\n",
    "                    tokens.append('Ġ'+word)\n",
    "        \n",
    "        for token in tokens:\n",
    "            if token in self.inv_vocab:\n",
    "                token_ids.append(self.inv_vocab[token])\n",
    "            else:\n",
    "                token_ids.extend(self.tokenize_with_bpe(token))\n",
    "        \n",
    "        return token_ids\n",
    "    \n",
    "    def tokenize_with_bpe(self, token):\n",
    "        # tokenize into individual characters\n",
    "        token_ids = [self.inv_vocab.get(ch, None) for ch in token]\n",
    "        \n",
    "        if None in token_ids:\n",
    "            missing_chars = [ch for ch, tok_id in zip(token, token_ids) if tok_id is None]\n",
    "            raise ValueError(f'Not found in vocab: {missing_chars}')\n",
    "        \n",
    "        # if GPT-2 merges aren't loaded\n",
    "        if not self.bpe_ranks:\n",
    "            can_merge = True\n",
    "            while can_merge and len(token_ids) > 1:\n",
    "                can_merge = False\n",
    "                new_tokens = []\n",
    "                i = 0\n",
    "\n",
    "                # N-1 because we access i+1\n",
    "                while i < len(token_ids)-1:\n",
    "                    pair = (token_ids[i], token_ids[i+1])\n",
    "                    \n",
    "                    if pair in self.bpe_merges:\n",
    "                        merged_id = self.bpe_merges[pair]\n",
    "                        new_tokens.append(merged_id)\n",
    "                        i += 2  # merged with next token, so skip it\n",
    "                        can_merge = True\n",
    "                    else:\n",
    "                        # (i, i+1) is not a pair in the merges\n",
    "                        new_tokens.append(token_ids[i])\n",
    "                        i += 1\n",
    "                \n",
    "                # last token ID\n",
    "                # this will be skipped if (N-1, N) was a merged pair in the previous loop\n",
    "                if i < len(token_ids):\n",
    "                    new_tokens.append(token_ids[i])\n",
    "                \n",
    "                # iterate until all merges are done\n",
    "                token_ids = new_tokens\n",
    "            \n",
    "            return token_ids\n",
    "        \n",
    "        def decode(self, token_ids):\n",
    "            decoded = \"\"\n",
    "            for i, token_id in enumerate(token_ids):\n",
    "                if token_id not in self.vocab:\n",
    "                    raise ValueError(f'Token ID {token_id} not in vocab')\n",
    "\n",
    "                token = self.vocab[token_id]\n",
    "                if token == '\\n':\n",
    "                    if decoded and not decoded.endswith(\" \"):\n",
    "                        decoded += \" \"  # add space before a newline\n",
    "                    decoded += token\n",
    "                elif token.startswith('Ġ'):\n",
    "                    # Ġ represented a space in BPE\n",
    "                    decoded += ' ' + token[1:]\n",
    "                else:\n",
    "                    decoded += token\n",
    "            \n",
    "            return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593c6143",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "code-book",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
